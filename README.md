# âœ‹ğŸ¤– Real-Time Sign Detection with MediaPipe & TensorFlow  

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg?logo=python&logoColor=white)]()
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg?logo=tensorflow)]()
[![OpenCV](https://img.shields.io/badge/OpenCV-Image%20Processing-green.svg?logo=opencv)]()
[![MediaPipe](https://img.shields.io/badge/MediaPipe-Holistic-red.svg)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)]()

---

## ğŸŒŸ Overview  

This repository implements a **real-time sign detection system** using **Googleâ€™s MediaPipe Holistic model** combined with a **TensorFlow deep learning classifier**.  

The system can:  
- ğŸ–ï¸ Detect **hand landmarks** (21 points per hand).  
- ğŸ‘¤ Track **pose landmarks** (body keypoints).  
- ğŸ˜€ Capture **face mesh landmarks** for additional context.  
- ğŸ” Train a classifier to recognize **hand signs or gestures** from landmark sequences.  
- ğŸ“¹ Operate in **real-time** on video streams using OpenCV.  

---

## ğŸ¯ Motivation  

Sign language is a critical communication tool, yet many ML datasets are **scarce** and **imbalanced**.  
This project aims to:  

- âœ… Provide an **accessible, real-time pipeline** for gesture/sign detection.  
- âœ… Combine **classical CV (OpenCV)** with **deep learning (TensorFlow)** and **pose estimation (MediaPipe)**.  
- âœ… Demonstrate **end-to-end training**, from landmark extraction â†’ dataset building â†’ model training â†’ real-time inference.  

---

## ğŸš€ Features  

- âš¡ **Real-time landmark detection** with MediaPipe.  
- âœï¸ **Custom dataset creation** from video feeds.  
- ğŸ§  **TensorFlow-based classification** of gestures.  
- ğŸ¥ **Live video demo** with overlayed landmarks + predictions.  
- ğŸ“Š **Scikit-learn utilities** for dataset splitting & evaluation.  

---

## ğŸ› ï¸ Technical Approach  

### ğŸ”¹ 1. Landmark Extraction with MediaPipe  
- Uses `mp.solutions.holistic.Holistic` for **face, hands, and pose landmarks**.  
- Each video frame is preprocessed with **OpenCV**:  
  - Convert **BGR â†’ RGB** for MediaPipe.  
  - Run through the Holistic model.  
  - Convert back **RGB â†’ BGR** for OpenCV display.  
- Landmarks are drawn using `mp.solutions.drawing_utils`.  

### ğŸ”¹ 2. Dataset Creation  
- Extracts landmark coordinates for:  
  - **21 hand landmarks (left & right)**  
  - **33 pose landmarks**  
  - **468 face landmarks (optional)**  
- Saves features + labels to disk for supervised training.  

### ğŸ”¹ 3. Model Training  
- Landmark arrays are flattened and fed into a **TensorFlow/Keras model**.  
- Typical architecture:  
  - Dense layers with ReLU activation.  
  - Dropout layers for regularization.  
  - Softmax output layer for multi-class classification.  
- Training process includes:  
  - Data normalization.  
  - Train/validation split via **scikit-learn**.  
  - Monitoring accuracy & loss.  

### ğŸ”¹ 4. Real-Time Inference  
- Capture live video stream with OpenCV.  
- Extract landmarks per frame with MediaPipe.  
- Preprocess and feed to the trained model.  
- Display predictions (e.g., â€œHelloâ€, â€œYesâ€, â€œNoâ€) directly on the video window.  

---

## ğŸ“Š Example Workflow  

```python
import cv2
import mediapipe as mp
from tensorflow.keras.models import load_model

# Load trained classifier
model = load_model("sign_classifier.h5")

# Initialize mediapipe holistic model
mp_holistic = mp.solutions.holistic
holistic = mp_holistic.Holistic()

cap = cv2.VideoCapture(0)

while cap.isOpened():
    ret, frame = cap.read()
    
    # Preprocess frame
    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = holistic.process(image)
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    
    # TODO: Extract landmarks â†’ preprocess â†’ predict with model
    
    cv2.imshow("Sign Detection", image)
    if cv2.waitKey(10) & 0xFF == ord("q"):
        break

cap.release()
cv2.destroyAllWindows()
